{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import asyncio\n",
    "import importlib\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Coroutine, Optional, Union\n",
    "\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "# from aiohttp import ClientError, ClientTimeout\n",
    "from bs4 import BeautifulSoup, NavigableString, ResultSet, Tag\n",
    "# from chompjs import parse_js_object, parse_js_objects\n",
    "from pandas import DataFrame\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from sqlalchemy import inspect\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "import json\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "def read_from_file(filepath: Path, parser: str) -> BeautifulSoup:\n",
    "    with filepath.open(mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        return BeautifulSoup(file.read(), features=parser)\n",
    "\n",
    "\n",
    "def write_to_file(filepath: Path, data: Union[str, dict], data_format: str = \"text\"):\n",
    "    try:\n",
    "        # Create a Path object from the provided file path\n",
    "        path = Path(filepath)\n",
    "\n",
    "        # Create parent directories if they don't exist\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if data_format == \"text\":\n",
    "            # If data is BeautifulSoup, convert it to a prettified string\n",
    "            if isinstance(data, BeautifulSoup):\n",
    "                data = str(data.prettify())\n",
    "\n",
    "            # Write the content to the file as text\n",
    "            with path.open(mode=\"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(data)\n",
    "        elif data_format == \"json\":\n",
    "            # If data is a dictionary, convert it to a JSON-formatted string\n",
    "            if isinstance(data, (dict, list)):\n",
    "                data = json.dumps(data, indent=4)\n",
    "\n",
    "            # Write the JSON content to the file\n",
    "            with path.open(mode=\"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(data)\n",
    "        elif data_format == \"md\":\n",
    "            with path.open(mode=\"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(data)            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid data format. Use 'text' or 'json'.\")\n",
    "\n",
    "        print(f\"Successfully wrote the data to '{filepath}' in {data_format} format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def soup(\n",
    "    source: Union[\n",
    "        BeautifulSoup, NavigableString, ResultSet, Tag, WebElement, WebDriver, str, Path\n",
    "    ],\n",
    "    parser=\"html.parser\",\n",
    "    filepath: Optional[Path | str] = \"temp.html\",\n",
    ") -> BeautifulSoup:\n",
    "    soupen = None\n",
    "\n",
    "    if isinstance(source, (BeautifulSoup, NavigableString, ResultSet, Tag)):\n",
    "        soupen = source\n",
    "    elif isinstance(source, Path) and source.exists():\n",
    "        soupen = read_from_file(source, parser)\n",
    "    elif isinstance(source, str):\n",
    "        path = Path(source)\n",
    "        if path.exists():\n",
    "            soupen = read_from_file(path, parser)\n",
    "        else:\n",
    "            soupen = BeautifulSoup(source, features=parser)\n",
    "    elif isinstance(source, WebElement):\n",
    "        source_html: str = source.get_attribute(\"innerHTML\")\n",
    "        soupen = BeautifulSoup(source_html, features=parser)\n",
    "    elif isinstance(source, WebDriver):\n",
    "        source_html: str = source.page_source\n",
    "        soupen = BeautifulSoup(source_html, features=parser)\n",
    "\n",
    "    if filepath and soupen is not None:\n",
    "        path = Path(filepath)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        write_to_file(path, soupen)\n",
    "\n",
    "    return soupen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _make_request(\n",
    "    session: aiohttp.ClientSession,\n",
    "    url: str,\n",
    "    headers: Optional[dict] = None,\n",
    "    cookies: Optional[dict] = None,\n",
    ") -> Any:\n",
    "    method = (\n",
    "        headers.get(\"method\").lower()\n",
    "        if isinstance(headers.get(\"method\"), str)\n",
    "        else None\n",
    "    )\n",
    "    SessionChoice = getattr(session, method) if method else session\n",
    "    async with SessionChoice(url, headers=headers, cookies=cookies) as response:\n",
    "        if response.status == 200:\n",
    "            # Check the content type of the response\n",
    "            content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "            # print(f\"{content_type=}\")\n",
    "            if \"application/json\" in content_type:\n",
    "                # If it's JSON, use .json() method\n",
    "                return dict(data=await response.json(), status=response.status)\n",
    "            else:\n",
    "                # Otherwise, return as text\n",
    "                return dict(data=await response.text(), status=response.status)\n",
    "        print(f\"Failed to fetch {url}. Status: {response.status}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def fetch(\n",
    "    session: aiohttp.ClientSession,\n",
    "    url: str,\n",
    "    retries=3,\n",
    "    timeout_for_wait: Optional[float] = None,\n",
    "    headers: Optional[dict] = None,\n",
    "    cookies: Optional[dict] = None,\n",
    ") -> Any:\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            return await asyncio.wait_for(\n",
    "                _make_request(session, url, headers, cookies),\n",
    "                timeout=timeout_for_wait,\n",
    "            )\n",
    "        except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "            if i == retries - 1:  # This was the last attempt\n",
    "                print(f\"Failed to fetch {url} after {retries} attempts. Error: {e}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "async def fetch_all(items, retries=3, timeout_for_session=5) -> list:\n",
    "    timeout = aiohttp.ClientTimeout(total=timeout_for_session)\n",
    "    async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "        tasks = [\n",
    "            fetch(\n",
    "                session,\n",
    "                item[\"url\"],\n",
    "                retries,\n",
    "                timeout_for_session,\n",
    "                item.get(\"headers\"),\n",
    "                item.get(\"cookies\"),\n",
    "            )\n",
    "            for item in items\n",
    "        ]\n",
    "        return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[30;1m2023-09-13 11:40:17\u001b[0m \u001b[34;1mINFO\u001b[0m \u001b[35minterface.backend.db - init - \u001b[95;1m28: \u001b[37;0mDb initialized.\n"
     ]
    }
   ],
   "source": [
    "from interface.backend import db\n",
    "\n",
    "await db.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jobsch_jobs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.models.jobsch.Job.__tablename__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL: str = \"https://www.jobs.ch/en/\"\n",
    "BASE_URL_JOBS: str = \"https://www.jobs.ch/en/vacancies/\"\n",
    "BASE_URL_COMPANY: str = \"https://www.jobs.ch/en/companies/\"\n",
    "BASE_URL_JOB_VAR: str = f\"{BASE_URL_JOBS}\" + \"detail/{0}\"\n",
    "BASE_URL_COMPANY_VAR: str = \"https://www.jobs.ch/en/companies/{0}/\"\n",
    "BASE_URL_JOBS_SEARCH: str = (\n",
    "    BASE_URL_JOBS + \"?location={location}&publication-date={pub_date}&term={term}\"\n",
    ")\n",
    "\n",
    "BASE_API_URL_COMPANY_VAR: str = \"https://www.jobs.ch/api/v1/public/company/{company_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows=20\n",
    "BASE_API_URL_JOBS_VAR = \"https://www.jobs.ch/api/v1/public/search?location={location}&query={query}&rows={rows}\"\n",
    "referer = \"https://www.jobs.ch/en/vacancies/?location=switzerland&term=python\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jobs.ch/en/vacancies/detail/7293f5a4-7b9c-4b36-bd92-829c260260d3-electrical-engineer-80-100'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_URL_JOB_VAR.format(\n",
    "    \"7293f5a4-7b9c-4b36-bd92-829c260260d3-electrical-engineer-80-100\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### domain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the domain approach\n",
    "async def extract_query_info(job_query: dict) -> dict[str, Any]:\n",
    "    return {\n",
    "        \"clientClassification\": job_query.get(\"botDetect\", {}).get(\n",
    "            \"clientClassification\"\n",
    "        ),\n",
    "        \"pageTitle\": job_query.get(\"seo\", {}).get(\"meta\", {}).get(\"pageTitle\"),\n",
    "        \"pageDescription\": job_query.get(\"seo\", {})\n",
    "        .get(\"meta\", {})\n",
    "        .get(\"pageDescription\"),\n",
    "        \"pageList\": str(\n",
    "            job_query.get(\"vacancy\", {}).get(\"traversal\", {}).get(\"pageList\")\n",
    "        ),\n",
    "        \"numPages\": job_query.get(\"vacancy\", {})\n",
    "        .get(\"results\", {})\n",
    "        .get(\"main\", {})\n",
    "        .get(\"meta\", {})\n",
    "        .get(\"numPages\"),\n",
    "        \"totalHits\": job_query.get(\"vacancy\", {})\n",
    "        .get(\"results\", {})\n",
    "        .get(\"main\", {})\n",
    "        .get(\"meta\", {})\n",
    "        .get(\"totalHits\"),\n",
    "        \"searchQuery\": job_query.get(\"vacancy\", {})\n",
    "        .get(\"results\", {})\n",
    "        .get(\"main\", {})\n",
    "        .get(\"meta\", {})\n",
    "        .get(\"searchQuery\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# queryInfo = extract_query_info(data_parsed[1])\n",
    "# queryInfo['pageList'] = str(queryInfo['pageList'])\n",
    "# query_df = pd.DataFrame(query_info).rename_axis('id')\n",
    "# query_df.index += 1\n",
    "# query_df\n",
    "# queryInfo\n",
    "\n",
    "\n",
    "async def handle_result(pkey, results, model, filepath):\n",
    "    data_parsed = await analyse_html(results[0], filepath)\n",
    "\n",
    "    queryInfo = await extract_query_info(data_parsed[1])\n",
    "\n",
    "    await update_query([pkey], [queryInfo], model)\n",
    "\n",
    "    return queryInfo, data_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_df.to_sql('query', db.engineSync, if_exists='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bot'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'585 jobs in Switzerland - jobs.ch'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Apply now for jobs in Switzerland! We have everything for your next job.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "585"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'location=Switzerland'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parsed[1][\"botDetect\"][\"clientClassification\"]\n",
    "data_parsed[1][\"seo\"][\"meta\"][\"pageTitle\"]\n",
    "data_parsed[1][\"seo\"][\"meta\"][\"pageDescription\"]\n",
    "data_parsed[1][\"vacancy\"][\"traversal\"][\"pageList\"]\n",
    "data_parsed[1][\"vacancy\"][\"results\"][\"main\"][\"meta\"][\"numPages\"]\n",
    "data_parsed[1][\"vacancy\"][\"results\"][\"main\"][\"meta\"][\"totalHits\"]\n",
    "data_parsed[1][\"vacancy\"][\"results\"][\"main\"][\"meta\"][\"searchQuery\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subqueries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subqueries_urls(base_url, numPages) -> list[str]:\n",
    "    return [f\"{base_url}&page={x}\" for x in range(1, numPages + 1)]\n",
    "\n",
    "\n",
    "async def generate_subqueries(queryInfo, queryPrimaryKey):\n",
    "    numPages = queryInfo[\"numPages\"]\n",
    "    base_url = (await db.retrieve_objs([queryPrimaryKey], db.models.Query))[0].url\n",
    "    if numPages > 1:\n",
    "        urls: list[str] = generate_subqueries_urls(base_url, numPages)\n",
    "        primaryIds: list[Any] = await db.create_objs(\n",
    "            db.models.SubQuery, [dict(url=url, queryId=queryPrimaryKey) for url in urls]\n",
    "        )\n",
    "    else:\n",
    "        primaryIds = [queryPrimaryKey]\n",
    "    return primaryIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def mainEn(\n",
    "    term=\"\",\n",
    "    location=\"Switzerland\",\n",
    "    pub_date=\"\",\n",
    "    headers: Optional[dict] = None,\n",
    "    cookies: Optional[dict] = None,\n",
    ") -> Coroutine[Any, Any, None]:\n",
    "    queryPrimaryKey = (await create_query(term, location, pub_date))[0][0]\n",
    "\n",
    "    items: list[dict[str, Any]] = [\n",
    "        {\n",
    "            \"url\": (await db.retrieve_objs([queryPrimaryKey], db.models.Query))[0].url,\n",
    "            \"name\": \"\",\n",
    "            \"headers\": {},\n",
    "            \"cookies\": {},\n",
    "            \"function\": None,  # This is a simple example function\n",
    "        },\n",
    "        # Add more dictionaries for more URLs\n",
    "    ]\n",
    "\n",
    "    results = await fetch_all(items)\n",
    "\n",
    "    if results:\n",
    "        queryInfo, data_parsed = await handle_result(\n",
    "            queryPrimaryKey,\n",
    "            results,\n",
    "            db.models.Query,\n",
    "            f\"./data/html/query/{queryPrimaryKey}.html\",\n",
    "        )\n",
    "\n",
    "        subQueriesPrimaryKeys: list[int] = await generate_subqueries(\n",
    "            queryInfo, queryPrimaryKey\n",
    "        )\n",
    "\n",
    "        # scrape each subQueriesPrimaryKeys\n",
    "        # fetch each subquery\n",
    "        # extract subQueryInfo\n",
    "        # extract Jobs(create jobs table with job+info, queryPm, subqueryPm)\n",
    "        # increment jobs in in subquery and query\n",
    "\n",
    "    return queryInfo, data_parsed, subQueriesPrimaryKeys\n",
    "    # return queryPrimaryKey, queryInfo, subQueriesPrimaryKeys\n",
    "\n",
    "\n",
    "queryInfo, data_parsed, subQueriesPrimaryKeys = await mainEn()\n",
    "\n",
    "# queryPrimaryKey, queryInfo, subqueriesPrimaryIds = await mainEn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subQueries = await db.retrieve_objs(subQueriesPrimaryKeys, db.models.SubQuery)\n",
    "for idx, subq in enumerate(subQueries, start=1):\n",
    "    items: list[dict[str, Any]] = [\n",
    "        {\n",
    "            \"url\": subq.url,\n",
    "            \"name\": \"\",\n",
    "            \"headers\": {},\n",
    "            \"cookies\": {},\n",
    "            \"function\": None,  # This is a simple example function\n",
    "        },\n",
    "        # Add more dictionaries for more URLs\n",
    "    ]\n",
    "    if result := await fetch_all(items):\n",
    "        queryInfo: dict[str, Any] = await handle_result(\n",
    "            subq.id,\n",
    "            result,\n",
    "            db.models.SubQuery,\n",
    "            f\"./data/html/subqueries/{subq.queryId}/{subq.id}.html\",\n",
    "        )\n",
    "\n",
    "    await asyncio.sleep(1)\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_LISTING_COLS: list[str] = [\n",
    "    \"title\",\n",
    "    \"id\",\n",
    "    \"initialPublicationDate\",\n",
    "    \"publicationDate\",\n",
    "    \"employmentGrades\",\n",
    "    \"preview\",\n",
    "    \"logo\",\n",
    "    \"isActive\",\n",
    "    \"languageSkills\",\n",
    "    \"place\",\n",
    "    \"salaryFrom\",\n",
    "    \"salaryTo\",\n",
    "    \"company.name\",\n",
    "    \"company.id\",\n",
    "    \"company.slug\",\n",
    "    \"company.visible\",\n",
    "    \"company.logoImage.src\",\n",
    "    \"company.logoImage.height\",\n",
    "    \"company.logoImage.width\",\n",
    "    \"company_url\",\n",
    "    \"job_url\",\n",
    "]\n",
    "\n",
    "\n",
    "def construnct_job_url(job_id) -> str:\n",
    "    return BASE_URL_JOB_VAR.format(job_id)\n",
    "\n",
    "\n",
    "def construnct_company_url(company_id) -> str:\n",
    "    return BASE_URL_COMPANY_VAR.format(company_id)\n",
    "\n",
    "\n",
    "def extract_job_info(job) -> dict[str, Any]:\n",
    "    return {\n",
    "        \"title\": job.get(\"title\", \"\"),\n",
    "        \"id\": job.get(\"id\", \"\"),\n",
    "        \"initialPublicationDate\": job.get(\"initialPublicationDate\", \"\"),\n",
    "        \"publicationDate\": job.get(\"publicationDate\", \"\"),\n",
    "        \"employmentGrades\": job.get(\"employmentGrades\", \"\"),\n",
    "        \"preview\": job.get(\"preview\", \"\"),\n",
    "        \"logo\": job.get(\"logo\", \"\"),\n",
    "        \"isActive\": job.get(\"isActive\", \"\"),\n",
    "        \"languageSkills\": job.get(\"languageSkills\", \"\"),\n",
    "        \"place\": job.get(\"place\", \"\"),\n",
    "        \"salaryFrom\": job.get(\"salaryFrom\", \"\"),\n",
    "        \"salaryTo\": job.get(\"salaryTo\", \"\"),\n",
    "        \"company_name\": job.get(\"company.name\", \"\"),\n",
    "        \"company_id\": job.get(\"company.id\", \"\"),\n",
    "        \"company_slug\": job.get(\"company.slug\", \"\"),\n",
    "        \"company_visible\": job.get(\"company.visible\", \"\"),\n",
    "        \"company_logoImage_src\": job.get(\"company.logoImage.src\", \"\"),\n",
    "        \"company_logoImage_height\": job.get(\"company.logoImage.height\", \"\"),\n",
    "        \"company_logoImage_width\": job.get(\"company.logoImage.width\", \"\"),\n",
    "        \"company_url\": construnct_company_url(job.get(\"company.id\", \"\")),\n",
    "        \"job_url\": construnct_job_url(job.get(\"id\", \"\")),\n",
    "    }\n",
    "\n",
    "\n",
    "async def scrape_jobs(data_parsed):\n",
    "    jobs_df: DataFrame = pd.json_normalize(\n",
    "        data_parsed[1][\"vacancy\"][\"results\"][\"main\"][\"results\"]\n",
    "    )\n",
    "    jobs_df.loc[:, \"company_url\"] = jobs_df[\"company.slug\"].agg(construnct_company_url)\n",
    "    jobs_df.loc[:, \"job_url\"] = jobs_df[\"id\"].agg(construnct_job_url)\n",
    "    jobs_df.loc[:, \"languageSkills\"] = jobs_df.loc[:, \"languageSkills\"].astype(str)\n",
    "\n",
    "    jobs_df[\"initialPublicationDate\"] = pd.to_datetime(\n",
    "        jobs_df[\"initialPublicationDate\"], format=\"%Y-%m-%dT%H:%M:%S%z\"\n",
    "    )\n",
    "    jobs_df[\"publicationDate\"] = pd.to_datetime(\n",
    "        jobs_df[\"publicationDate\"], format=\"%Y-%m-%dT%H:%M:%S%z\"\n",
    "    )\n",
    "\n",
    "    jobs: DataFrame = jobs_df.loc[:, JOB_LISTING_COLS].set_index(\"id\")\n",
    "\n",
    "    job_dicts = [extract_job_info(job[1]) for job in jobs.iterrows()]\n",
    "    return await db.create_objs(db.models.Job, job_dicts)\n",
    "\n",
    "\n",
    "await scrape_jobs(data_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"2023-08-04T17:06:49+02:00\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parsed[1][\"vacancy\"][\"results\"][\"main\"][\"results\"]\n",
    "# write_string_to_txt_file(\"data_parsed.py\", data_parsed)\n",
    "with open(\"jobs.py\", \"w\") as file:\n",
    "    # Pretty print the list and write it to the file\n",
    "    pprint.pprint(_, stream=file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['applicationQuestions', 'benefits', 'categories', 'contactAddress',\n",
       "       'contactDetails', 'contactPerson', 'contacts', 'employmentGrades',\n",
       "       'employmentGradesList', 'employmentPositionIds', 'employmentTypeIds',\n",
       "       'headhunterApplicationAllowed', 'id', 'initialPublicationDate',\n",
       "       'isActive', 'isInsecureExternal', 'isWebVacancy', 'languageSkills',\n",
       "       'layoutType', 'listingTags', 'locations', 'offerId', 'place', 'preview',\n",
       "       'publicationDate', 'regions', 'salaryFrom', 'salaryTo',\n",
       "       'sourceHostname', 'title', 'videos', 'regionId', 'displayType',\n",
       "       'company.id', 'company.name', 'company.slug', 'company.visible', 'logo',\n",
       "       'company.companyLogoFile', 'company.logoImage.src',\n",
       "       'company.logoImage.height', 'company.logoImage.width', 'mood'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_df: DataFrame = pd.json_normalize(\n",
    "    data_parsed[1][\"vacancy\"][\"results\"][\"main\"][\"results\"]\n",
    ")\n",
    "jobs_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_listing_cols: list[str] = [\n",
    "    \"title\",\n",
    "    \"id\",\n",
    "    \"initialPublicationDate\",\n",
    "    \"publicationDate\",\n",
    "    \"employmentGrades\",\n",
    "    \"preview\",\n",
    "    \"logo\",\n",
    "    \"isActive\",\n",
    "    \"languageSkills\",\n",
    "    \"place\",\n",
    "    \"salaryFrom\",\n",
    "    \"salaryTo\",\n",
    "    \"company.name\",\n",
    "    \"company.id\",\n",
    "    \"company.slug\",\n",
    "    \"company.visible\",\n",
    "    \"company.logoImage.src\",\n",
    "    \"company.logoImage.height\",\n",
    "    \"company.logoImage.width\",\n",
    "    \"company_url\",\n",
    "    \"job_url\",\n",
    "]\n",
    "\n",
    "\n",
    "def construnct_job_url(job_id) -> str:\n",
    "    return BASE_URL_JOB_VAR.format(job_id)\n",
    "\n",
    "\n",
    "def construnct_company_url(company_id) -> str:\n",
    "    return BASE_URL_COMPANY_VAR.format(company_id)\n",
    "\n",
    "\n",
    "jobs_df.loc[:, \"company_url\"] = jobs_df[\"company.slug\"].agg(construnct_company_url)\n",
    "jobs_df.loc[:, \"job_url\"] = jobs_df[\"id\"].agg(construnct_job_url)\n",
    "jobs_df.loc[:, \"languageSkills\"] = jobs_df.loc[:, \"languageSkills\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs: DataFrame = jobs_df.loc[:, job_listing_cols].set_index(\"id\")\n",
    "jobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the dataframe to a table named 'users'\n",
    "jobs.drop([\"languageSkills\"], axis=1).to_sql(\"jobs2\", engineSync, if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_info(job) -> dict[str, Any]:\n",
    "    return {\n",
    "        \"title\": job.get(\"title\", \"\"),\n",
    "        \"id\": job.get(\"id\", \"\"),\n",
    "        \"initialPublicationDate\": job.get(\"initialPublicationDate\", \"\"),\n",
    "        \"publicationDate\": job.get(\"publicationDate\", \"\"),\n",
    "        \"employmentGrades\": job.get(\"employmentGrades\", \"\"),\n",
    "        \"preview\": job.get(\"preview\", \"\"),\n",
    "        \"logo\": job.get(\"logo\", \"\"),\n",
    "        \"isActive\": job.get(\"isActive\", \"\"),\n",
    "        \"languageSkills\": job.get(\"languageSkills\", \"\"),\n",
    "        \"place\": job.get(\"place\", \"\"),\n",
    "        \"salaryFrom\": job.get(\"salaryFrom\", \"\"),\n",
    "        \"salaryTo\": job.get(\"salaryTo\", \"\"),\n",
    "        \"company.name\": job.get(\"company.name\", \"\"),\n",
    "        \"company.id\": job.get(\"company.id\", \"\"),\n",
    "        \"company.slug\": job.get(\"company.slug\", \"\"),\n",
    "        \"company.visible\": job.get(\"company.visible\", \"\"),\n",
    "        \"company.logoImage.src\": job.get(\"company.logoImage.src\", \"\"),\n",
    "        \"company.logoImage.height\": job.get(\"company.logoImage.height\", \"\"),\n",
    "        \"company.logoImage.width\": job.get(\"company.logoImage.width\", \"\"),\n",
    "        \"company.url\": construnct_company_url(job.get(\"company.id\", \"\")),\n",
    "        \"url\": construnct_job_url(job.get(\"id\", \"\")),\n",
    "    }\n",
    "\n",
    "\n",
    "# extract_job_info(jobs_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summer Camps Junior Coaches - Various Activities'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'c031309d-69c4-4efc-bcc9-e66b8f3b80d3'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'2023-07-04T10:06:55+02:00'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'2023-08-03T01:58:57+02:00'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'80% – 100%'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Foundation \\nLa Grande Boissière/ La Châtaigneraie/ Nations \\nEcolint Camps Department \\nSeeks for Summer Camps 2023 and/or Half term holidays ...'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Geneva, Switzerland'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Ecolint'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'78430'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'78430-ecolint'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_df.iloc[0][\"title\"]\n",
    "jobs_df.iloc[0][\"id\"]\n",
    "jobs_df.iloc[0][\"initialPublicationDate\"]\n",
    "jobs_df.iloc[0][\"publicationDate\"]\n",
    "jobs_df.iloc[0][\"employmentGrades\"]\n",
    "jobs_df.iloc[0][\"preview\"]\n",
    "jobs_df.iloc[0][\"logo\"]\n",
    "jobs_df.iloc[0][\"isActive\"]\n",
    "jobs_df.iloc[0][\"languageSkills\"]\n",
    "jobs_df.iloc[0][\"place\"]\n",
    "jobs_df.iloc[0][\"salaryFrom\"]\n",
    "jobs_df.iloc[0][\"salaryTo\"]\n",
    "jobs_df.iloc[0][\"company.name\"]\n",
    "jobs_df.iloc[0][\"company.id\"]\n",
    "jobs_df.iloc[0][\"company.slug\"]\n",
    "jobs_df.iloc[0][\"company.visible\"]\n",
    "jobs_df.iloc[0][\"company.logoImage.src\"]\n",
    "jobs_df.iloc[0][\"company.logoImage.height\"]\n",
    "jobs_df.iloc[0][\"company.logoImage.width\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is only needed for the domain url not the api\n",
    "async def analyse_html(html: str, filepath: str):\n",
    "    html_soup: BeautifulSoup = soup(source=html, filepath=filepath)\n",
    "\n",
    "    script_tag: Tag | NavigableString | None = html_soup.find(\n",
    "        \"script\",\n",
    "        string=lambda x: all(\n",
    "            text_string in x\n",
    "            for text_string in [\n",
    "                \"__INIT__\",\n",
    "                \"pageTitle\",\n",
    "                \"pageDescription\",\n",
    "                \"numPages\",\n",
    "                \"totalHits\",\n",
    "            ]\n",
    "        )\n",
    "        if x is not None\n",
    "        else False,\n",
    "    )\n",
    "\n",
    "    if script_tag:\n",
    "        data_parsed = [x for x in list(parse_js_objects(script_tag.string)) if x]\n",
    "        return data_parsed\n",
    "    else:\n",
    "        print(\"Script tag was not found in the html.\")\n",
    "\n",
    "\n",
    "# debug\n",
    "# write_string_to_txt_file(\"scrapers/jobsdotch/temp.txt\", script_tag.string)\n",
    "\n",
    "\n",
    "# debug\n",
    "# write_string_to_txt_file(\"data_parsed.py\", data_parsed)\n",
    "# with open(\"scrapers/jobsdotch/data_parsed.py\", \"w\") as file:\n",
    "# Pretty print the list and write it to the file\n",
    "# pprint.pprint(data_parsed, stream=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapers.jobsdotch import QueryBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_request(items, filepath):\n",
    "    data_json_list = await fetch_all(items)\n",
    "    if data_json := data_json_list[0]:\n",
    "        write_to_file(filepath, data_json.get(\"data\", {}), \"json\")\n",
    "        return data_json\n",
    "\n",
    "\n",
    "async def create_query(query, location, days, url, model):\n",
    "    return await db.create_objs(\n",
    "        model,\n",
    "        [dict(query=query, location=location, days=days, url=url)],\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_subqueries_urls(base_url, numPages) -> list[str]:\n",
    "    return [f\"{base_url}&page={x}\" for x in range(1, numPages + 1)]\n",
    "\n",
    "\n",
    "async def generate_subqueries(\n",
    "    num_pages, queryPrimaryKey, query, location, days, model_retrieve, model_create\n",
    "):\n",
    "    base_url = (await db.retrieve_objs([queryPrimaryKey], model_retrieve))[0].url\n",
    "    if num_pages > 1:\n",
    "        urls: list[str] = generate_subqueries_urls(base_url, num_pages)\n",
    "        return await db.create_objs(\n",
    "            model_create,\n",
    "            [\n",
    "                dict(\n",
    "                    url=url,\n",
    "                    qpkey=queryPrimaryKey,\n",
    "                    query=query,\n",
    "                    location=location,\n",
    "                    days=days,\n",
    "                    page=page,\n",
    "                )\n",
    "                for page, url in enumerate(urls, start=1)\n",
    "            ],\n",
    "        )\n",
    "    else:\n",
    "        return [queryPrimaryKey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_query_info(rsp: dict):\n",
    "    return dict(\n",
    "        num_pages=rsp.get(\"data\", {}).get(\"num_pages\", \"\"),\n",
    "        current_page=rsp.get(\"data\", {}).get(\"current_page\", \"\"),\n",
    "        total_hits=rsp.get(\"data\", {}).get(\"total_hits\", \"\"),\n",
    "        actual_hits=len(rsp.get(\"data\", {}).get(\"documents\", [])),\n",
    "        status=rsp.get(\"status\", None),\n",
    "    )\n",
    "\n",
    "\n",
    "# len(rsp.get('documents',''))\n",
    "\n",
    "# await extract_query_info(results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_job_info(job: dict):\n",
    "    return {\n",
    "        \"title\": job.get(\"title\", \"\"),\n",
    "        \"publication_date\": job.get(\"publication_date\", \"\"),\n",
    "        \"company_name\": job.get(\"company_name\", \"\"),\n",
    "        \"place\": job.get(\"place\", \"\"),\n",
    "        \"is_active\": job.get(\"is_active\", \"\"),\n",
    "        \"preview\": job.get(\"preview\", \"\"),\n",
    "        \"company_logo_file\": job.get(\"company_logo_file\", \"\"),\n",
    "        \"job_id\": job.get(\"job_id\", \"\"),\n",
    "        \"slug\": job.get(\"slug\", \"\"),\n",
    "        \"company_slug\": job.get(\"company_slug\", \"\"),\n",
    "        \"company_id\": job.get(\"company_id\", \"\"),\n",
    "        \"company_segmentation\": job.get(\"company_segmentation\", \"\"),\n",
    "        \"employment_position_ids\": str(job.get(\"employment_position_ids\", [])),\n",
    "        \"employment_grades\": str(job.get(\"employment_grades\", [])),\n",
    "        \"is_paid\": job.get(\"is_paid\", \"\"),\n",
    "        \"work_experience\": str(job.get(\"work_experience\", [])),\n",
    "        \"language_skills\": str(job.get(\"language_skills\", [])),\n",
    "        \"job_url_en\": job.get(\"_links\").get(\"detail_de\").get(\"href\", \"\"),\n",
    "        \"job_url_de\": job.get(\"_links\").get(\"detail_fr\").get(\"href\", \"\"),\n",
    "        \"job_url_fr\": job.get(\"_links\").get(\"detail_en\").get(\"href\", \"\")\n",
    "        # \"links\":job.get('_links', []),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.json_normalize(results[0]['documents'][0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jobs.ch/api/v1/public/search?location=Switzerland&query=python'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryBuilder(\n",
    "    query=\"python\",\n",
    "    location=\"Switzerland\",\n",
    ").url_api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote the data to 'data/json/queries/1.json' in json format.\n",
      "Successfully wrote the data to 'data/json/sub_queries/1/1.json' in json format.\n",
      "Successfully wrote the data to 'data/json/sub_queries/1/2.json' in json format.\n"
     ]
    }
   ],
   "source": [
    "# block duplicate jobs and count their number, add a duplicate col in jobs\n",
    "async def mainEn(\n",
    "    query=\"\",\n",
    "    location=\"Switzerland\",\n",
    "    days=\"\",\n",
    "    headers: Optional[dict] = None,\n",
    "    cookies: Optional[dict] = None,\n",
    ") -> Coroutine[Any, Any, None]:\n",
    "    if headers is None:\n",
    "        headers = {}\n",
    "    if cookies is None:\n",
    "        cookies = {}\n",
    "\n",
    "    # add full parameters\n",
    "    query_url = QueryBuilder(query=\"python\", location=\"Switzerland\", days=\"\").url_api\n",
    "\n",
    "    query_pk = (\n",
    "        await create_query(query, location, days, query_url, db.models.Query_Api)\n",
    "    )[0][0]\n",
    "\n",
    "    items: list[dict[str, Any]] = [\n",
    "        {\n",
    "            \"url\": query_url,\n",
    "            \"name\": \"\",\n",
    "            \"headers\": headers,\n",
    "            \"cookies\": cookies,\n",
    "        },\n",
    "        # Add more dictionaries for more URLs\n",
    "    ]\n",
    "\n",
    "    data_json = await handle_request(items, f\"data/json/queries/{query_pk}.json\")\n",
    "\n",
    "    if data_json.get(\"data\", {}):\n",
    "        query_info = await extract_query_info(data_json)\n",
    "\n",
    "        await db.update_objs([query_pk], db.models.Query_Api, [query_info])\n",
    "\n",
    "        subqueries_pkeys = await generate_subqueries(\n",
    "            query_info[\"num_pages\"],\n",
    "            query_pk,\n",
    "            query,\n",
    "            location,\n",
    "            days,\n",
    "            db.models.Query_Api,\n",
    "            db.models.SubQuery_Api,\n",
    "        )\n",
    "\n",
    "        for spkey in subqueries_pkeys:\n",
    "            spkey = spkey[0]\n",
    "            sub_data_json = await handle_request(\n",
    "                items, f\"data/json/sub_queries/{query_pk}/{spkey}.json\"\n",
    "            )\n",
    "            sub_query_info = await extract_query_info(sub_data_json)\n",
    "            await db.update_objs([spkey], db.models.SubQuery_Api, [sub_query_info])\n",
    "\n",
    "            for job in sub_data_json.get(\"data\", {}).get(\"documents\", {}):\n",
    "                db.duplicate_finder()\n",
    "                job_info = await extract_job_info(job)\n",
    "                job_info[\"query_id\"] = query_pk\n",
    "                job_info[\"subquery_id\"] = spkey\n",
    "                await db.create_objs(db.models.Job_Api, [job_info])\n",
    "\n",
    "                # add duplicate check\n",
    "\n",
    "    return (\n",
    "        query_pk,\n",
    "        data_json,\n",
    "        query_info,\n",
    "        spkey,\n",
    "        sub_data_json,\n",
    "        sub_query_info,\n",
    "    )\n",
    "\n",
    "\n",
    "query_pk, data_json, query_info, spkey, sub_data_json, sub_query_info = await mainEn()\n",
    "\n",
    "# queryPrimaryKey, queryInfo, subqueriesPrimaryIds = await mainEn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
